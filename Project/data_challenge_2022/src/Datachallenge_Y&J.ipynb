{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='center' ><font size='70'>Data Challenge</font></div>\n",
    "\n",
    "\n",
    "<center>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "YANG Yining & JIN Zhongwei\n",
    "\n",
    "10 Avril, 2022\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yangyining/nltk_data\n",
      "/Users/yangyining/opt/anaconda3/envs/pytorch/nltk_data\n",
      "/Users/yangyining/opt/anaconda3/envs/pytorch/share/nltk_data\n",
      "/Users/yangyining/opt/anaconda3/envs/pytorch/lib/nltk_data\n",
      "/usr/share/nltk_data\n",
      "/usr/local/share/nltk_data\n",
      "/usr/lib/nltk_data\n",
      "/usr/local/lib/nltk_data\n"
     ]
    }
   ],
   "source": [
    "# packages for NLP\n",
    "import nltk\n",
    "for path in nltk.data.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 -- Task Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link prediction is the problem of predicting a potential link between 2 entities within the network. In the problem, we are given a citation network consisting of 138499 papers, along with their abstracts and arthors. Moreover, one list file of 1091955 existing edges is given. However, they are incomplete. We are asked to predict whether test edges existed and the probability of their existence.\n",
    "\n",
    "Here, we divide our workflow into several <u>tasks</u>:\n",
    "- Feature engineering: Explorating and constructing the features of edges\n",
    "- Model construction & training\n",
    "- Evaluations & test results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../.DS_Store\n",
      "../submission.csv\n",
      "../Docs/.DS_Store\n",
      "../Docs/data_challenge_handout.pdf\n",
      "../data/edgelist.txt\n",
      "../data/abstracts.txt\n",
      "../data/test.txt\n",
      "../data/authors.txt\n",
      "../data/generated_data/abs_w2v_model_withmincount1.pkl\n",
      "../data/generated_data/athrs_w2v_model.pkl\n",
      "../data/generated_data/authors.pkl\n",
      "../data/generated_data/abstract_w.pkl\n",
      "../data/generated_data/tr4w.pkl\n",
      "../data/generated_data/abs_w2v_model.pkl\n",
      "../src/text_baseline.py\n",
      "../src/Text_authors_preprocessing.py\n",
      "../src/graph_baseline.py\n",
      "../src/Text_abstracts_preprocessing.py\n",
      "../src/Datachallenge_Y&J.ipynb\n",
      "../src/__pycache__/Text_abstracts_preprocessing.cpython-37.pyc\n",
      "../src/__pycache__/Text_authors_preprocessing.cpython-37.pyc\n"
     ]
    }
   ],
   "source": [
    "# print the file tree\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 -- Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these part, our aim is to extract information from different dimentions of data. \n",
    "\n",
    "There are 2 main source of edge features:\n",
    "- From ``graph`` structure: degrees, ranks,...\n",
    "- Converting the paper (<u>node</u> in the graph) attributes including ``authors`` and ``abstracts`` into citation (<u>edge</u> in the graph) features: similarities, ...\n",
    "\n",
    "For the first part, we could get them directly when the graph is established. For the next part, we should do some natural language preprocessing work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part, we could read the ``graph`` information and use the intrinsic properties in the given graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_edgelist('../data/edgelist.txt', delimiter=',',\n",
    "                     create_using=nx.Graph(), nodetype=int)  ## import the graph from the edgelist file.\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138498"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the language processing, we are going to perform different tricks on ``abstracts`` and ``authors`` texts.\n",
    "\n",
    "> ### abstracts.txt\n",
    ">1. Using the ``TextRank`` algorithm to extract keywords from natural sentences of an abstract. (https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0)\n",
    ">2. Establish the ``Word2Vec`` representation of abstract words for articles. (dimension of 100)\n",
    ">3. Create article features using the ``keyword vectors`` (10 keywords).\n",
    "\n",
    "> ### authors.txt\n",
    ">1. Establish the ``Word2Vec`` representation of authors for papers. (dimension of 100)\n",
    ">2. Create the article features using the ``author vectors``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the definition of the textrank algorithm which is defined in the Text_abstracts_preprocessing.py\n",
    "#import Text_abstracts_preprocessing \n",
    "from Text_abstracts_preprocessing import TextRankForKeyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Text_abstracts_preprocessing import abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the testranked\n",
    "with open('../data/generated_data/tr4w.pkl','rb') as abstracts_read:\n",
    "  tr4w_input_abstracts = pickle.load(abstracts_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr4w_abstracts = dict()\n",
    "for node in tr4w_input_abstracts:\n",
    "  tr4w_abstracts[node] = pickle.loads(tr4w_input_abstracts[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "# for node in abstracts:\n",
    "#   print(tr4w_abstracts[node].sentence_segment(nlp(abstracts[node]),candidate_pos=['NOUN', 'PROPN', 'ADJ'],lower =True))\n",
    "#   if node==3:\n",
    "#     break\n",
    "\n",
    "### it is the word extraction function for word2vec embedding\n",
    "### It is too long! It is recommended to load the result from pickle file\n",
    "\n",
    "abstract_data=[tr4w_abstracts[node].sentence_segment(nlp(abstracts[node]),candidate_pos=['NOUN', 'PROPN', 'ADJ'],lower =True) for node in G.nodes()]\n",
    "\n",
    "# # store the abstract_data into .pkl file\n",
    "# with open('../data/generated_data/abstract_w.pkl','wb') as abstract_data_file:\n",
    "#   pickle.dump(abstract_data,abstract_data_file)\n",
    "#   abstract_data_file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/generated_data/abstract_w.pkl','rb') as abstract_data_file:\n",
    "  abstract_data = pickle.load(abstract_data_file)\n",
    "  abstract_data_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_w2v_model_data = list()\n",
    "for node in abstracts:\n",
    "  for sent in abstract_data[node]:\n",
    "    abs_w2v_model_data.append(sent)\n",
    "\n",
    "\n",
    "# with open('../data/generated_data/abstract_w2v_model_input.pkl','wb') as abstract_inputdata_file:\n",
    "#   pickle.dump(abstract_data,abstract_inputdata_file)\n",
    "#   abstract_inputdata_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41580969, 45747750)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_w2v_model = Word2Vec(size=100, window=5, min_count=1, sg=0, workers=8)\n",
    "abs_w2v_model.build_vocab(abs_w2v_model_data)\n",
    "abs_w2v_model.train(abs_w2v_model_data, total_examples=abs_w2v_model.corpus_count, epochs=5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the model into .pkl file\n",
    "with open('../data/generated_data/abs_w2v_model_withmincount1.pkl','wb') as abs_w2v_model_file:\n",
    "  model_bytes = pickle.dumps(abs_w2v_model)\n",
    "  pickle.dump(model_bytes,abs_w2v_model_file)\n",
    "  abs_w2v_model_file.close()\n",
    "\n",
    "# with open('../data/generated_data/abs_w2v_model_withmincount1.pkl','rb') as abs_w2v_model_file:\n",
    "#   abs_w2v_model = pickle.load(abs_w2v_model_file)\n",
    "#   abs_w2v_model = pickle.loads(abs_w2v_model)\n",
    "#   abs_w2v_model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The development of an automated system for the quality assessment of aerodrome ground lighting (AGL), in accordance with associated standards and recommendations, is presented. The system is composed of an image sensor, placed inside the cockpit of an aircraft to record images of the AGL during a normal descent to an aerodrome. A model-based methodology is used to ascertain the optimum match between a template of the AGL and the actual image data in order to calculate the position and orientation of the camera at the instant the image was acquired. The camera position and orientation data are used along with the pixel grey level for each imaged luminaire, to estimate a value for the luminous intensity of a given luminaire. This can then be compared with the expected brightness for that luminaire to ensure it is operating to the required standards. As such, a metric for the quality of the AGL pattern is determined. Experiments on real image data is presented to demonstrate the application and effectiveness of the system.\\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_w2v_model.wv.get_vector('agl')==abs_w2v_model.wv.word_vec('agl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47216004"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws1 = list([i.lower() for i in OrderedDict(sorted(tr4w_abstracts[0].node_weight.items(), key=lambda t: t[1], reverse=True))])\n",
    "ws2 = list([i.lower() for i in OrderedDict(sorted(tr4w_abstracts[100].node_weight.items(), key=lambda t: t[1], reverse=True))])\n",
    "\n",
    "abs_w2v_model.wv.n_similarity(ws1[:10],ws2[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 10, 10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.array([abs_w2v_model.wv.get_vector(i).reshape((10,10)) for i in ws1[:8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10, 10)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.array(np.broadcast_to(np.mean([abs_w2v_model.wv.get_vector(i).reshape((10,10)) for i in ws1[:8]],axis =0),(3,10,10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_abstract_feature(G,Word2Vec=abs_w2v_model,key_generator =tr4w_abstracts, f_reshape = (10,10),num_keyword=10):\n",
    "  feature = np.zeros((len(G.nodes),f_reshape[0],f_reshape[1],num_keyword))\n",
    "  for n in G.nodes():\n",
    "    #print(n)\n",
    "    ## generate list of all keyword vectors (the order is depend on the keyword's significance)\n",
    "    keyws = list([i.lower() for i in OrderedDict(sorted(key_generator[n].node_weight.items(), key=lambda t: t[1], reverse=True))])\n",
    "    \n",
    "    ## pick num_keyword vectors to construct the ndarray of shape (f_reshape,num_keyword): typically ((10,10),10)\n",
    "    if len(keyws)<num_keyword:\n",
    "      if len(keyws)>0:\n",
    "        origi_feature = np.zeros((num_keyword,f_reshape[0],f_reshape[1]))\n",
    "        origi_feature[:len(keyws)]=np.array([Word2Vec.wv.get_vector(i).reshape(f_reshape) for i in keyws[:]])\n",
    "        origi_feature[len(keyws):]=np.array(np.broadcast_to(np.mean([abs_w2v_model.wv.get_vector(i).reshape(f_reshape) for i in keyws[:len(keyws)]],axis =0),(num_keyword-len(keyws),f_reshape[0],f_reshape[1])))\n",
    "        feature[n]=origi_feature.transpose((1, -1, 0))\n",
    "      else:\n",
    "        feature[n]=np.zeros((f_reshape[0],f_reshape[1],num_keyword))\n",
    "    else:\n",
    "      feature[n] = np.array([Word2Vec.wv.get_vector(i).reshape(f_reshape) for i in keyws[:num_keyword]]).transpose((1, -1, 0))\n",
    "\n",
    "  return feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_abs_feature = node_abstract_feature(G,num_keyword=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138499, 10, 10, 8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_abs_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mabs_w2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Compute cosine similarity between two sets of words.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "ws1 : list of str\n",
      "    Sequence of words.\n",
      "ws2: list of str\n",
      "    Sequence of words.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "numpy.ndarray\n",
      "    Similarities between `ws1` and `ws2`.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/gensim/models/keyedvectors.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "abs_w2v_model.wv.n_similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'development': 0.6090520682324654,\n",
       " 'automated': 0.7086640311498161,\n",
       " 'system': 1.5679570113392165,\n",
       " 'quality': 1.3171029580107905,\n",
       " 'assessment': 1.0329315988838061,\n",
       " 'aerodrome': 1.428328959770789,\n",
       " 'ground': 1.0717865363074939,\n",
       " 'lighting': 1.0710650930750285,\n",
       " 'AGL': 2.84224636207708,\n",
       " 'accordance': 0.8263990018051872,\n",
       " 'standard': 0.9720223325738512,\n",
       " 'recommendation': 0.6346649158719633,\n",
       " 'image': 2.883652050757183,\n",
       " 'sensor': 0.7287399050369255,\n",
       " 'cockpit': 0.8164182291826758,\n",
       " 'aircraft': 0.89771077463189,\n",
       " 'normal': 0.7176374172831043,\n",
       " 'descent': 0.639638563678444,\n",
       " 'model': 0.6301721618431856,\n",
       " 'methodology': 0.7577006521962233,\n",
       " 'optimum': 0.8455715132389162,\n",
       " 'match': 0.9517206957597248,\n",
       " 'template': 1.0520358132679348,\n",
       " 'actual': 0.9577995187961232,\n",
       " 'datum': 2.0465024726228793,\n",
       " 'order': 0.8585879852028008,\n",
       " 'position': 1.1875646813677716,\n",
       " 'orientation': 1.199074135114456,\n",
       " 'camera': 1.0010472345501968,\n",
       " 'instant': 0.5636257638358058,\n",
       " 'pixel': 0.908579658383817,\n",
       " 'grey': 0.9362909118573551,\n",
       " 'level': 0.9599273988736912,\n",
       " 'imaged': 0.9799074069522983,\n",
       " 'luminaire': 1.6213407042476753,\n",
       " 'value': 0.9334071239257538,\n",
       " 'luminous': 0.8436643068469991,\n",
       " 'intensity': 0.7475630305191604,\n",
       " 'brightness': 0.4349339391377853,\n",
       " 'metric': 0.5523252264641489,\n",
       " 'pattern': 0.5523252264641489,\n",
       " 'experiment': 0.5810782393763094,\n",
       " 'real': 0.6686385328902668,\n",
       " 'application': 0.7793395430357042,\n",
       " 'effectiveness': 0.6832583135611557}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr4w_abstracts[0].node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('graphs', 0.8177399635314941),\n",
       " ('hypergraph', 0.7595082521438599),\n",
       " ('subgraph', 0.694742739200592),\n",
       " ('vertex', 0.677985668182373),\n",
       " ('bipartite', 0.6227419376373291),\n",
       " ('undirected', 0.6220051050186157),\n",
       " ('clique', 0.6205345988273621),\n",
       " ('dag', 0.6048083901405334),\n",
       " ('hyperedge', 0.5968008637428284),\n",
       " ('node', 0.5712962746620178)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_w2v_model.wv.most_similar('graph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to deal with the author data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Text_authors_preprocessing import authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456810"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(authors[node]) for node in range(len(authors))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "distin_authors = set()\n",
    "for node in range(len(authors)):\n",
    "  distin_authors = distin_authors|set(authors[node])\n",
    "\n",
    "# with open(\"../data/generated_data/authors.pkl\",'wb') as authorfile:\n",
    "#   pickle.dump(distin_authors,authorfile)\n",
    "#   authorfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce the distinct authors set\n",
    "with open(\"../data/generated_data/authors.pkl\",'rb') as authorfile:\n",
    "  distin_authors=pickle.load(authorfile)\n",
    "  authorfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 149683 distinct authors occured in these articles\n"
     ]
    }
   ],
   "source": [
    "print('There are %d distinct authors occured in these articles'%len(distin_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_word_data = [list(authors[node]) for node in authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2284050, 2284050)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athrs_w2v_model = Word2Vec(size=100, window=5, min_count=1, sg=0, workers=8)  # Here we set the min_count frequency to 1 because the authors are sparse to occur\n",
    "athrs_w2v_model.build_vocab(authors_word_data)\n",
    "athrs_w2v_model.train(authors_word_data, total_examples=athrs_w2v_model.corpus_count, epochs=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the model into .pkl file\n",
    "with open('../data/generated_data/athrs_w2v_model.pkl','wb') as athrs_w2v_model_file:\n",
    "  atrs_model_bytes = pickle.dumps(athrs_w2v_model)\n",
    "  pickle.dump(atrs_model_bytes,athrs_w2v_model_file)\n",
    "  athrs_w2v_model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce the model in .pkl file\n",
    "with open('../data/generated_data/athrs_w2v_model.pkl', 'rb') as athrs_w2v_model_file:\n",
    "    athrs_w2v_model = pickle.load(athrs_w2v_model_file)\n",
    "    athrs_w2v_model = pickle.loads(athrs_w2v_model)\n",
    "    athrs_w2v_model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'George W. Irwin', 'James H. Niblock', 'Jian-Xun Peng', 'Karen R. McMenemy'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.069632575"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athrs_w2v_model.wv.similarity('Jian-Xun Peng','James H. Niblock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_author_feature(G,Word2Vec=athrs_w2v_model,author_data =authors_word_data, f_reshape = (10,10),num_author=2):\n",
    "  feature = np.zeros((len(G.nodes),f_reshape[0],f_reshape[1],num_author))\n",
    "  for n in G.nodes():\n",
    "    #print(n)\n",
    "\n",
    "    ## pick num_keyword vectors to construct the ndarray of shape (f_reshape,num_keyword): typically ((10,10),10)\n",
    "    if len(author_data[n])<num_author:\n",
    "      if len(author_data[n])>0:\n",
    "        origi_feature = np.zeros((num_author,f_reshape[0],f_reshape[1]))\n",
    "        origi_feature[:len(author_data[n])]=np.array([Word2Vec.wv.get_vector(i).reshape(f_reshape) for i in author_data[n][:]])\n",
    "        if len(author_data[n])==1:\n",
    "          origi_feature[len(author_data[n]):]=np.array(np.broadcast_to(Word2Vec.wv.get_vector(author_data[n][0]).reshape(f_reshape),(num_author-len(author_data[n]),f_reshape[0],f_reshape[1])))\n",
    "        else:\n",
    "          origi_feature[len(author_data[n]):]=np.array(np.broadcast_to(np.mean([Word2Vec.wv.get_vector(i).reshape(f_reshape) for i in author_data[n]],axis =0),(num_author-len(author_data[n]),f_reshape[0],f_reshape[1])))\n",
    "        feature[n]=origi_feature.transpose((1, -1, 0))\n",
    "      else:\n",
    "        feature[n]=np.zeros((f_reshape[0],f_reshape[1],num_author))\n",
    "    else:\n",
    "      feature[n] = np.array([Word2Vec.wv.get_vector(i).reshape(f_reshape) for i in author_data[n][:num_author]]).transpose((1, -1, 0))\n",
    "\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_athrs_feature = node_author_feature(G,Word2Vec=athrs_w2v_model,num_author=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined feature dimensions: (138499, 10, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "print('The combined feature dimensions:',np.shape(np.concatenate((G_abs_feature,G_athrs_feature),axis=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_feature(features = (G_abs_feature,G_athrs_feature),ax=3):\n",
    "  return np.concatenate(features,axis=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = node_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/generated_data/X.npy',X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138499, 10, 10, 10)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7659de0085ded71da1fb33be0dd6a6a94986ee1dac5ead8473599c206b3e3c92"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
